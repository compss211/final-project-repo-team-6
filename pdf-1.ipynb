{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c72bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 14:59:38,529 - INFO - Downloading: https://www.congress.gov/54/crecb/1897/01/14/GPO-CRECB-1897-pt1-v29-22-1.pdf\n",
      "2025-10-26 14:59:39,754 - INFO - Downloaded to: temp_pdfs/sample.pdf\n",
      "2025-10-26 14:59:39,756 - INFO - Processing: sample.pdf\n",
      "2025-10-26 14:59:43,301 - INFO - Results saved to ./congressional_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONGRESSIONAL RECORD EXTRACTION REPORT\n",
      "======================================================================\n",
      "Total documents processed: 1\n",
      "Successful extractions: 1\n",
      "Failed extractions: 0\n",
      "\n",
      "Field coverage:\n",
      "  Dates extracted: 1 (100.0%)\n",
      "  Congress sessions: 0 (0.0%)\n",
      "  Chambers identified: 1 (100.0%)\n",
      "  Total bills mentioned: 60\n",
      "  Total speakers identified: 20\n",
      "  Total committees mentioned: 10\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Congressional Record/Bill Data Extractor\n",
    "Extract structured data from Congressional PDFs (congress.gov)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('congressional_extraction.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CongressionalExtractor:\n",
    "    \"\"\"Extract structured data from Congressional Record PDFs\"\"\"\n",
    "    \n",
    "    def __init__(self, output_file: str = \"congressional_data.csv\"):\n",
    "        self.output_file = output_file\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "        \n",
    "    def download_pdf(self, url: str, save_path: str) -> bool:\n",
    "        \"\"\"Download a PDF from a URL\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Downloading: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            logger.info(f\"Downloaded to: {save_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download {url}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_date(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract date from Congressional Record\"\"\"\n",
    "        # Look for dates like \"THURSDAY, January 14, 1897\"\n",
    "        patterns = [\n",
    "            r'((?:MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY|SUNDAY),\\s+\\w+\\s+\\d{1,2},\\s+\\d{4})',\n",
    "            r'(\\w+ \\d{1,2}, \\d{4})',\n",
    "            r'(\\d{1,2}/\\d{1,2}/\\d{4})',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def extract_congress_session(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract Congress session number\"\"\"\n",
    "        # Look for patterns like \"54th Congress\" or \"Fifty-fourth Congress\"\n",
    "        patterns = [\n",
    "            r'(\\d{1,3})(?:st|nd|rd|th)\\s+Congress',\n",
    "            r'(Fifty-fourth|Fifty-third|etc\\.)\\s+Congress',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def extract_chamber(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Determine if Senate or House\"\"\"\n",
    "        if re.search(r'\\bSENATE\\b', text[:500], re.IGNORECASE):\n",
    "            return \"Senate\"\n",
    "        elif re.search(r'\\bHOUSE\\b', text[:500], re.IGNORECASE):\n",
    "            return \"House\"\n",
    "        return None\n",
    "    \n",
    "    def extract_bill_numbers(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract all bill numbers mentioned\"\"\"\n",
    "        # Patterns: H.R. 9710, S. 3526, etc.\n",
    "        patterns = [\n",
    "            r'\\b([HS]\\.?\\s?R\\.?\\s?\\d+)\\b',\n",
    "            r'\\b([HS]\\.?\\s?J\\.?\\s?Res\\.?\\s?\\d+)\\b',\n",
    "            r'\\b(S\\.\\s?\\d+)\\b',\n",
    "        ]\n",
    "        \n",
    "        bill_numbers = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            bill_numbers.extend(matches)\n",
    "        \n",
    "        # Clean and deduplicate\n",
    "        cleaned = list(set([b.strip().upper() for b in bill_numbers]))\n",
    "        return cleaned\n",
    "    \n",
    "    def extract_speakers(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract speaker names\"\"\"\n",
    "        # Pattern: \"Mr. SMITH.\" or \"The VICE-PRESIDENT.\"\n",
    "        pattern = r'(?:Mr\\.|Mrs\\.|Miss|Ms\\.|The)\\s+([A-Z\\-]+)[\\.\\:]'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        # Deduplicate and clean\n",
    "        speakers = list(set([s.strip() for s in matches if len(s) > 2]))\n",
    "        return speakers[:20]  # Limit to first 20 unique speakers\n",
    "    \n",
    "    def extract_committees(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract committee names\"\"\"\n",
    "        pattern = r'Committee on ([\\w\\s,]+?)(?:\\.|;|,\\s+to)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        # Clean and deduplicate\n",
    "        committees = list(set([c.strip() for c in matches]))\n",
    "        return committees[:10]  # Limit to first 10\n",
    "    \n",
    "    def extract_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract main topics/subjects discussed\"\"\"\n",
    "        topics = []\n",
    "        \n",
    "        # Look for common topic indicators\n",
    "        topic_patterns = [\n",
    "            r'(?:relating to|concerning|regarding)\\s+([\\w\\s]{10,50})',\n",
    "            r'(?:bill|resolution).*?(?:to|for)\\s+([\\w\\s]{10,50})',\n",
    "        ]\n",
    "        \n",
    "        for pattern in topic_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            topics.extend(matches)\n",
    "        \n",
    "        # Clean and limit\n",
    "        topics = list(set([t.strip() for t in topics if len(t) > 10]))\n",
    "        return topics[:5]\n",
    "    \n",
    "    def extract_votes(self, text: str) -> Dict:\n",
    "        \"\"\"Extract voting information\"\"\"\n",
    "        votes = {\n",
    "            'has_votes': False,\n",
    "            'passed': None,\n",
    "            'agreed_to': None\n",
    "        }\n",
    "        \n",
    "        if re.search(r'The motion was agreed to', text):\n",
    "            votes['has_votes'] = True\n",
    "            votes['agreed_to'] = True\n",
    "        \n",
    "        if re.search(r'The motion was (?:not )?agreed to', text):\n",
    "            votes['has_votes'] = True\n",
    "        \n",
    "        if re.search(r'(?:passed|agreed to|adopted)', text, re.IGNORECASE):\n",
    "            votes['passed'] = True\n",
    "        \n",
    "        return votes\n",
    "    \n",
    "    def extract_from_pdf(self, pdf_path: Path, source_url: str = None) -> Dict:\n",
    "        \"\"\"Extract all relevant information from a Congressional PDF\"\"\"\n",
    "        logger.info(f\"Processing: {pdf_path.name}\")\n",
    "        \n",
    "        result = {\n",
    "            'filename': pdf_path.name,\n",
    "            'source_url': source_url,\n",
    "            'date': None,\n",
    "            'congress_session': None,\n",
    "            'chamber': None,\n",
    "            'bill_numbers': [],\n",
    "            'num_bills': 0,\n",
    "            'speakers': [],\n",
    "            'num_speakers': 0,\n",
    "            'committees': [],\n",
    "            'num_committees': 0,\n",
    "            'topics': [],\n",
    "            'has_votes': False,\n",
    "            'page_count': 0,\n",
    "            'status': 'success',\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                result['page_count'] = len(pdf.pages)\n",
    "                \n",
    "                # Extract text from all pages\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\"\n",
    "                \n",
    "                # Extract fields\n",
    "                result['date'] = self.extract_date(full_text)\n",
    "                result['congress_session'] = self.extract_congress_session(full_text)\n",
    "                result['chamber'] = self.extract_chamber(full_text)\n",
    "                result['bill_numbers'] = self.extract_bill_numbers(full_text)\n",
    "                result['num_bills'] = len(result['bill_numbers'])\n",
    "                result['speakers'] = self.extract_speakers(full_text)\n",
    "                result['num_speakers'] = len(result['speakers'])\n",
    "                result['committees'] = self.extract_committees(full_text)\n",
    "                result['num_committees'] = len(result['committees'])\n",
    "                result['topics'] = self.extract_topics(full_text)\n",
    "                \n",
    "                # Extract vote information\n",
    "                vote_info = self.extract_votes(full_text)\n",
    "                result.update(vote_info)\n",
    "                \n",
    "                # Store lists as strings for CSV\n",
    "                result['bill_numbers'] = '; '.join(result['bill_numbers'])\n",
    "                result['speakers'] = '; '.join(result['speakers'])\n",
    "                result['committees'] = '; '.join(result['committees'])\n",
    "                result['topics'] = '; '.join(result['topics'])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path.name}: {e}\")\n",
    "            result['status'] = 'error'\n",
    "            result['error'] = str(e)\n",
    "            self.errors.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_url_list(self, url_file: str):\n",
    "        \"\"\"Process a list of URLs from a file\"\"\"\n",
    "        with open(url_file, 'r') as f:\n",
    "            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]\n",
    "        \n",
    "        logger.info(f\"Found {len(urls)} URLs to process\")\n",
    "        \n",
    "        # Create temp directory for downloads\n",
    "        temp_dir = Path(\"./temp_pdfs\")\n",
    "        temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        for idx, url in enumerate(urls, 1):\n",
    "            logger.info(f\"Progress: {idx}/{len(urls)}\")\n",
    "            \n",
    "            # Generate filename from URL\n",
    "            filename = url.split('/')[-1]\n",
    "            if not filename.endswith('.pdf'):\n",
    "                filename = f\"document_{idx}.pdf\"\n",
    "            \n",
    "            pdf_path = temp_dir / filename\n",
    "            \n",
    "            # Download PDF\n",
    "            if self.download_pdf(url, str(pdf_path)):\n",
    "                # Extract data\n",
    "                result = self.extract_from_pdf(pdf_path, url)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                # Clean up downloaded file\n",
    "                try:\n",
    "                    pdf_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Save checkpoint every 100 files\n",
    "            if idx % 100 == 0:\n",
    "                self.save_results(f\"checkpoint_{idx}.csv\")\n",
    "        \n",
    "        logger.info(f\"Completed processing {len(urls)} URLs\")\n",
    "    \n",
    "    def process_directory(self, pdf_directory: str):\n",
    "        \"\"\"Process all PDFs in a directory\"\"\"\n",
    "        pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {pdf_directory}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "        \n",
    "        for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "            logger.info(f\"Progress: {idx}/{len(pdf_files)}\")\n",
    "            result = self.extract_from_pdf(pdf_path)\n",
    "            self.results.append(result)\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                self.save_results(f\"checkpoint_{idx}.csv\")\n",
    "    \n",
    "    def save_results(self, filename: Optional[str] = None):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        output_file = filename or self.output_file\n",
    "        \n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to save\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "        if self.errors:\n",
    "            error_df = pd.DataFrame(self.errors)\n",
    "            error_file = output_file.replace('.csv', '_errors.csv')\n",
    "            error_df.to_csv(error_file, index=False)\n",
    "            logger.info(f\"Errors saved to {error_file}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a summary report\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CONGRESSIONAL RECORD EXTRACTION REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total documents processed: {len(self.results)}\")\n",
    "        print(f\"Successful extractions: {len(df[df['status'] == 'success'])}\")\n",
    "        print(f\"Failed extractions: {len(df[df['status'] == 'error'])}\")\n",
    "        print(f\"\\nField coverage:\")\n",
    "        print(f\"  Dates extracted: {df['date'].notna().sum()} ({df['date'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Congress sessions: {df['congress_session'].notna().sum()} ({df['congress_session'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Chambers identified: {df['chamber'].notna().sum()} ({df['chamber'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Total bills mentioned: {df['num_bills'].sum()}\")\n",
    "        print(f\"  Total speakers identified: {df['num_speakers'].sum()}\")\n",
    "        print(f\"  Total committees mentioned: {df['num_committees'].sum()}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = CongressionalExtractor(\n",
    "        output_file=\"./congressional_data.csv\"\n",
    "    )\n",
    "    \n",
    "    # OPTION 1: Process a single URL (for testing)\n",
    "    single_url = \"https://www.congress.gov/54/crecb/1897/01/14/GPO-CRECB-1897-pt1-v29-22-1.pdf\"\n",
    "    temp_dir = Path(\"./temp_pdfs\")\n",
    "    temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "    temp_pdf = temp_dir / \"sample.pdf\"\n",
    "    \n",
    "    if extractor.download_pdf(single_url, str(temp_pdf)):\n",
    "        result = extractor.extract_from_pdf(temp_pdf, single_url)\n",
    "        extractor.results.append(result)\n",
    "        try:\n",
    "            temp_pdf.unlink()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # OPTION 2: Process from a file containing URLs (one per line)\n",
    "    # Uncomment the line below and comment out OPTION 1 to use this\n",
    "    # extractor.process_url_list(\"./urls.txt\")\n",
    "    \n",
    "    # OPTION 3: Process PDFs already downloaded to a directory\n",
    "    # Uncomment the line below and comment out OPTION 1 to use this\n",
    "    # extractor.process_directory(\"./pdfs\")\n",
    "    \n",
    "    # Save and report\n",
    "    extractor.save_results()\n",
    "    extractor.generate_report()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
